{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¦´ Vertebrae Segmentation Pipeline - Kaggle Training\n",
    "\n",
    "This notebook trains the complete 3-stage pipeline using **CLI commands**.\n",
    "\n",
    "## Steps:\n",
    "1. Clone repository from GitHub\n",
    "2. Install dependencies\n",
    "3. Run training commands for each stage\n",
    "4. Export trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install SimpleITK nibabel tensorboard scipy pandas scikit-learn tqdm --quiet\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURE YOUR GITHUB REPOSITORY\n",
    "# ============================================================\n",
    "GITHUB_REPO = \"https://github.com/YOUR_USERNAME/YOUR_REPO.git\"  # <-- CHANGE THIS\n",
    "\n",
    "REPO_NAME = GITHUB_REPO.split('/')[-1].replace('.git', '')\n",
    "\n",
    "%cd /kaggle/working\n",
    "\n",
    "# Clone if not exists\n",
    "if not os.path.exists(REPO_NAME):\n",
    "    !git clone {GITHUB_REPO}\n",
    "    print(f\"âœ… Cloned {REPO_NAME}\")\n",
    "else:\n",
    "    print(f\"âœ… Repository exists, pulling latest...\")\n",
    "    %cd {REPO_NAME}\n",
    "    !git pull\n",
    "    %cd ..\n",
    "\n",
    "# Add to Python path\n",
    "PIPELINE_DIR = f\"/kaggle/working/{REPO_NAME}\"\n",
    "sys.path.insert(0, PIPELINE_DIR)\n",
    "\n",
    "# Add to sys.path for imports\n",
    "sys.path.insert(0, PIPELINE_DIR)\n",
    "\n",
    "!ls {REPO_NAME}/\n",
    "# Verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš ï¸ Important: Commit Changes to GitHub\n",
    "\n",
    "Before running on Kaggle, make sure you've committed the updated `train.py` and `infer.py` files to GitHub.\n",
    "\n",
    "These files have been updated to use **absolute imports** instead of relative imports, which is required for the code to run properly.\n",
    "\n",
    "```bash\n",
    "git add train.py infer.py\n",
    "git commit -m \"Fix imports for direct script execution\"\n",
    "git push origin main\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURE PATHS\n",
    "# ============================================================\n",
    "DATASET_CSV = \"/kaggle/input/verse19/dataset.csv\"  # <-- CHANGE THIS\n",
    "OUTPUT_DIR = \"/kaggle/working/outputs\"\n",
    "FOLD = 0\n",
    "NUM_FOLDS = 5\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "# Training epochs\n",
    "EPOCHS_STAGE1 = 100\n",
    "EPOCHS_STAGE2 = 150\n",
    "EPOCHS_STAGE3 = 100\n",
    "BATCH_SIZE = 2\n",
    "LR = 0.0001\n",
    "\n",
    "# ============================================================\n",
    "# QUICK TEST MODE\n",
    "# ============================================================\n",
    "QUICK_TEST = True  # <-- Set to False for full training\n",
    "\n",
    "if QUICK_TEST:\n",
    "    EPOCHS_STAGE1 = 3\n",
    "    EPOCHS_STAGE2 = 3\n",
    "    EPOCHS_STAGE3 = 3\n",
    "    print(\"âš ï¸ QUICK TEST MODE: 3 epochs per stage\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Dataset: {DATASET_CSV}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ðŸš€ Stage 1: Spine Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%cd {PIPELINE_DIR}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STAGE 1: SPINE LOCALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Import and run training\n",
    "sys.path.insert(0, PIPELINE_DIR)\n",
    "import train\n",
    "\n",
    "# Set command-line arguments\n",
    "sys.argv = [\n",
    "    'train.py',\n",
    "    '--stage', 'spine',\n",
    "    '--csv', DATASET_CSV,\n",
    "    '--output', OUTPUT_DIR,\n",
    "    '--fold', str(FOLD),\n",
    "    '--num_folds', str(NUM_FOLDS),\n",
    "    '--epochs', str(EPOCHS_STAGE1),\n",
    "    '--batch_size', str(BATCH_SIZE),\n",
    "    '--lr', str(LR),\n",
    "    '--device', DEVICE\n",
    "]\n",
    "\n",
    "# Run training\n",
    "train.main()\n",
    "\n",
    "print(\"\\nâœ… Stage 1 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ðŸš€ Stage 2: Vertebrae Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STAGE 2: VERTEBRAE LOCALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Set command-line arguments\n",
    "sys.argv = [\n",
    "    'train.py',\n",
    "    '--stage', 'vertebrae',\n",
    "    '--csv', DATASET_CSV,\n",
    "    '--output', OUTPUT_DIR,\n",
    "    '--fold', str(FOLD),\n",
    "    '--num_folds', str(NUM_FOLDS),\n",
    "    '--epochs', str(EPOCHS_STAGE2),\n",
    "    '--batch_size', str(BATCH_SIZE),\n",
    "    '--lr', str(LR),\n",
    "    '--device', DEVICE\n",
    "]\n",
    "\n",
    "# Run training\n",
    "train.main()\n",
    "\n",
    "print(\"\\nâœ… Stage 2 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ðŸš€ Stage 3: Vertebrae Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"STAGE 3: VERTEBRAE SEGMENTATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Set command-line arguments\n",
    "sys.argv = [\n",
    "    'train.py',\n",
    "    '--stage', 'segmentation',\n",
    "    '--csv', DATASET_CSV,\n",
    "    '--output', OUTPUT_DIR,\n",
    "    '--fold', str(FOLD),\n",
    "    '--num_folds', str(NUM_FOLDS),\n",
    "    '--epochs', str(EPOCHS_STAGE3),\n",
    "    '--batch_size', str(BATCH_SIZE),\n",
    "    '--lr', str(LR),\n",
    "    '--device', DEVICE\n",
    "]\n",
    "\n",
    "# Run training\n",
    "train.main()\n",
    "\n",
    "print(\"\\nâœ… Stage 3 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ðŸ§ª Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(DATASET_CSV)\n",
    "TEST_IMAGE = df.iloc[0]['image_path']\n",
    "\n",
    "print(f\"Testing inference on: {TEST_IMAGE}\")\n",
    "\n",
    "# Import and run inference\n",
    "import infer\n",
    "\n",
    "# Set command-line arguments\n",
    "sys.argv = [\n",
    "    'infer.py',\n",
    "    '--image', TEST_IMAGE,\n",
    "    '--model_dir', OUTPUT_DIR,\n",
    "    '--output', f'{OUTPUT_DIR}/inference_test',\n",
    "    '--device', DEVICE,\n",
    "    '--save_intermediates'\n",
    "]\n",
    "\n",
    "# Run inference\n",
    "infer.main()\n",
    "\n",
    "print(\"\\nâœ… Inference Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "zip_path = f\"/kaggle/working/trained_models_fold{FOLD}\"\n",
    "shutil.make_archive(zip_path, 'zip', OUTPUT_DIR)\n",
    "\n",
    "print(f\"âœ… Created: {zip_path}.zip\")\n",
    "print(f\"ðŸ“¥ Download from Output tab!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9322895,
     "sourceId": 14595189,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6454328,
     "sourceId": 10414190,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
